# Endpoint for P2P node to listen on
p2p-endpoint = 0.0.0.0:1776

# P2P nodes to connect to on startup (may specify multiple times)
checkpoint = []

# JSON array of P2P nodes to connect to on startup
# seed-nodes = 

# Pairs of [BLOCK_NUM,BLOCK_ID] that should be enforced as checkpoints.
# checkpoint = 

# Endpoint for websocket RPC to listen on
rpc-endpoint =  0.0.0.0:8090 

# Endpoint for TLS websocket RPC to listen on
 #rpc-tls-endpoint = 127.0.0.1:8089

# The TLS certificate file for this server
 server-pem = 

# Password for this certificate
 #server-pem-password = 

# File to read Genesis State from
# genesis-json = /etc/bitshares/genesis.json

# Block signing key to use for init witnesses, overrides genesis file
# dbg-init-key = 

# JSON file specifying API permissions
# api-access = 

# Space-separated list of plugins to activate
# plugins = 

# Enable block production, even if the chain is stale.
enable-stale-production = true

# Percent of witnesses (0-99) that must be participating in order to produce blocks
required-participation = 0

# Tuple of [PublicKey, WIF private key] (for account that will get rewards for being an activenode)

# ID of witness controlled by this node (e.g. "1.6.5", quotes are required, may specify multiple times)
witness-id = "1.6.0"
witness-id = "1.6.1"
witness-id = "1.6.2"
witness-id = "1.6.3"
witness-id = "1.6.4"
witness-id = "1.6.5"
witness-id = "1.6.6"
witness-id = "1.6.7"
witness-id = "1.6.8"
witness-id = "1.6.9"
witness-id = "1.6.10"



# Tuple of [PublicKey, WIF private key] (may specify multiple times)
 private-key = ["KES6MRyAjQq8ud7hVNYcfnVPJqcVpscN5So8BhtHuGYqET5GDW5CV","5KQwrPbwdL6PhXujxW37FSSQZ1JiwsST4cqQzDeyXtP79zkvFD3"]

# Path to a file containing tuples of [PublicKey, WIF private key]. The file has to contain exactly one tuple (i.e. private - public key pair) per line. This option may be specified multiple times, thus multiple files can be provided.
# private-key-file =


# ==============================================================================
# debug_witness plugin options
# ==============================================================================

# Tuple of [PublicKey, WIF private key] (may specify multiple times)
 debug-private-key = ["KES6MRyAjQq8ud7hVNYcfnVPJqcVpscN5So8BhtHuGYqET5GDW5CV","5KQwrPbwdL6PhXujxW37FSSQZ1JiwsST4cqQzDeyXtP79zkvFD3"]

# Account ID to track history for (may specify multiple times)

# Account ID to track history for (may specify multiple times)
# track-account = 

# A random number that will be used by a pseudo-random number generator as a source of entropy
# user-provided-seed =
# ./get_dev_key PRODUCTION  productionkey6 productionkey7 productionkey8 productionkey9 productionkey10 productionkey11

#import_key nathan "5KQwrPbwdL6PhXujxW37FSSQZ1JiwsST4cqQzDeyXtP79zkvFD3"

#import_balance nathan ["5KQwrPbwdL6PhXujxW37FSSQZ1JiwsST4cqQzDeyXtP79zkvFD3"] true

# Keep only those operations in memory that are related to account history tracking
partial-operations = 1

# Maximum number of operations per account will be kept in memory
max-ops-per-account = 1000

# Elastic Search database node url
# elasticsearch-node-url = 

# Number of bulk documents to index on replay(5000)
# elasticsearch-bulk-replay = 

# Number of bulk documents to index on a syncronied chain(10)
# elasticsearch-bulk-sync = 

# Log bulk events to database
# elasticsearch-logs = 

# Use visitor to index additional data(slows down the replay)
# elasticsearch-visitor = 

# Track market history by grouping orders into buckets of equal size measured in seconds specified as a JSON array of numbers
bucket-size = [60,300,900,1800,3600,14400,86400]

# How far back in time to track history for each bucket size, measured in the number of buckets (default: 1000)
history-per-size = 1000

# Will only store this amount of matched orders for each market in order history for querying, or those meet the other option, which has more data (default: 1000)
max-order-his-records-per-market = 1000

# Will only store matched orders in last X seconds for each market in order history for querying, or those meet the other option, which has more data (default: 259200 (3 days))
max-order-his-seconds-per-market = 259200

# RPC endpoint of a trusted validating node (required)
# trusted-node = 

# Block number after which to do a snapshot
# snapshot-at-block = 

# Block time (ISO format) after which to do a snapshot
# snapshot-at-time = 

# Pathname of JSON file where to store the snapshot
# snapshot-to = 

# Elasticsearch node url
# es-objects-elasticsearch-url = 

# Log bulk events to database
# es-objects-logs = 

# Number of bulk documents to index on replay(5000)
# es-objects-bulk-replay = 

# Number of bulk documents to index on a syncronied chain(10)
# es-objects-bulk-sync = 

# Store proposal objects
# es-objects-proposals = 

# Store account objects
# es-objects-accounts = 

# Store asset objects
# es-objects-assets = 

# Store balances objects
# es-objects-balances = 

# Store limit order objects
# es-objects-limit-orders = 

# Store feed data
# es-objects-asset-bitasset = 

# Group orders by percentage increase on price. Specify a JSON array of numbers here, each number is a group, number 1 means 0.01%. 
tracked-groups = [10,100]

# declare an appender named "stderr" that writes messages to the console
[log.console_appender.stderr]
stream=std_error

# declare an appender named "p2p" that writes messages to p2p.log
[log.file_appender.p2p]
# filename can be absolute or relative to this config file
filename=logs/p2p/p2p.log
# Rotate log every ? minutes, if leave out default to 60
rotation_interval=60
# how long will logs be kept (in days), if leave out default to 7
rotation_limit=1

# route any messages logged to the default logger to the "stderr" logger we
# declared above, if they are info level are higher
[logger.default]
level=info
appenders=stderr

# route messages sent to the "p2p" logger to the p2p appender declared above
[logger.p2p]
level=info
appenders=p2p

